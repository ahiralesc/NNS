#!/usr/bin/env python

# Copyright Hash-LSH Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


#########################################################################    ##
#
# Author: Adan Hirales Carbajal
# Email : adan.hirales@cetys.mx
#
#########################################################################    ##


import argparse
import pandas as pd
import os
from normalization import FilterFactory, loadRawData, writeRawData


hp = {
		"filters" 	: [],
		"source"	: None,
		"target"	: None,
		"verbose"	: False
	}


def parseCLA( ):
	parser = argparse.ArgumentParser(description='Applies text normalization filters in the order in which filters are specified to the target text file.')

	parser.add_argument('-rmcc', nargs='?', default=-1, type=int, help='Replace unicode control characters [ \\t\\n\\r\\f\\v] with a single white space.')
	parser.add_argument('-rmpc', nargs='?', default=-1, type=int, help='Replace punctuation characters [!"#$%%&\'()*+, -./:;<=>?@[\]^_`{|}~]')
	parser.add_argument('-rmnc', nargs='?', default=-1, type=int, help='Remove numeric characters [0-9]')
	parser.add_argument('-rmsc', nargs='?', default=-1, type=int, help='Remove special characters except: a-zA-Z0-9 áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ:')
	parser.add_argument('-ttlc', nargs='?', default=-1, type=int, help='Transform strings to lower case')
	parser.add_argument('-regex', nargs='?', default=-1, type=int, help='Label string patterns that match regular expressions')
	parser.add_argument('-s', nargs='?', default=None, type=str, help='The source text file')
	parser.add_argument('-t', nargs='?', default=None, type=str, help='The target filtered text file')
	parser.add_argument('-v', nargs='?', default=False, type=bool, help='Verbose output');

	args = vars(parser.parse_args())

	# Extract switches and sort them
	switches = dict(list(args.items())[:6])
	switches = {k:v for k, v in sorted(switches.items(), key=lambda v: int(v[1]))}

	for k, v in switches.items():
		if v != -1:
			hp["filters"].append(k)

	# Extract source and target file name and location
	hp["source"] = args['s']
	hp["target"] = args['t']
	hp["verbose"] = args['v']

	if (hp["source"] == None or hp["target"] == None):
		print("source and/or target files not specified")
		exit(0);



def loadSource( ):
	files = list()
	for file in os.listdir( hp["source"] ):
		key, val = file.split(".")
		if val == "txt" or val == "TXT":
			files.append( file )
	return files



def normalize( source, target  ):
	# data loading phase
	pdf = pd.DataFrame( data = { 'data': [ loadRawData(source) ] }, index=[0] )

	# filter creation phase
	filters = dict()
	filter_factory = FilterFactory()
	for name in hp["filters"]:
		print("Creating filter : ", name )
		filters[name] = filter_factory.create(name)

	# Data normalization phase
	for _name, _filter in filters.items():
		pdf = _filter.apply(pdf,"data")

	# Storage phase
	writeRawData( target, pdf.loc[0,'data'] )
	


# main thread of execution
# 1. parser command line arguments
parseCLA( )
# 2. load the file dataset 
files = loadSource()
# 3. normalize files in the dataset
for file in files:
	source = hp["source"] + file
	target = hp["target"] + file
	if hp['verbose'] == True :
		print("Normalizing {}, writting output to {}".format(file, target))
	normalize( source, target )

